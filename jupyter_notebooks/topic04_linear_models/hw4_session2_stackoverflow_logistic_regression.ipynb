{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению. Сессия № 2\n",
    "Авторы материала: Павел Нестеров. Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Домашняя работа №4\n",
    "## <center> Логистическая регрессия в задаче тегирования вопросов StackOverflow\n",
    "\n",
    "**Надо вывести формулы, где это просится (да, ручка и бумажка), заполнить код в клетках и выбрать ответы в [веб-форме](https://docs.google.com/forms/d/1I_ticU8rpeoGJjsBUcaInpvgdxdq60hV7IcSvo4rlGo/).**\n",
    "\n",
    "## 0. Описание задачи\n",
    "\n",
    "В этой домашней работе мы с вами изучим и запрограммируем модель для прогнозирования тегов по тексту вопроса на базе многоклассовой логистической регрессии. В отличие от обычной постановки задачи классификации (multiclass), в данном случае один пример может принадлежать одновременно к нескольким классам (multilabel). Мы будем реализовывать онлайн версию алгоритма мультиклассовой классификации.\n",
    "\n",
    "Мы будем использовать небольшую выборку из протеггированных вопросов с сайта StackOverflow размером в 125 тысяч примеров (около 150 Мб, скачайте по [этой](https://drive.google.com/open?id=0B4bl7YMqDnViYVo0V2FubFVhMFE) ссылке).\n",
    "\n",
    "PS: Можно показать, что такая реализация совсем не эффективная и проще было бы использовать векторизированные вычисления. Для данного датасета так и есть. Но на самом деле подобные реализации используются в жизни, но естественно, написаны они не на Python. Например, в онлайн моделях прогнозирования [CTR](https://en.wikipedia.org/wiki/Click-through_rate) юзеру показывается баннер, затем в зависимости от наличия клика происходит обновление параметров модели. В реальной жизни параметров модели может быть несколько сотен миллионов, а у юзера из этих ста миллионов от силы сто или тысяча параметров отличны от нуля, векторизировать такие вычисления не очень эффективно. Обычно все это хранится в огромных кластерах в in-memory базах данных, а обработка пользователей происходит распределенно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "sns.set(font='DejaVu Sans')\n",
    "\n",
    "# поменяйте на свой путь\n",
    "DS_FILE_NAME = \"../../data/stackoverflow/stackoverflow_sample_125k.tsv\"\n",
    "TAGS_FILE_NAME = \"../../data/stackoverflow/top10_tags.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python', 'javascript', 'html', 'java', 'ios', 'android', 'php', 'c++', 'jquery', 'c#'}\n"
     ]
    }
   ],
   "source": [
    "top_tags = []\n",
    "\n",
    "with open(TAGS_FILE_NAME, 'r') as f:\n",
    "    for line in f:\n",
    "        top_tags.append(line.strip())\n",
    "\n",
    "top_tags = set(top_tags)\n",
    "\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Многоклассовая логистическая регрессия\n",
    "\n",
    "Вспомним, как получается логистическая регрессия для двух классов $\\left\\{0, 1\\right\\}$, вероятность принадлежности объекта к классу $1$ выписывается по теореме Байеса:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = 1 \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right) + p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} \\\\\n",
    "&=& \\dfrac{1}{1 + e^{-a}} \\\\\n",
    "&=& \\sigma\\left(a\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\vec{x}$ – вектор признаков объекта\n",
    "- $\\sigma$ – обозначение функции логистического сигмоида при скалярном аргументе\n",
    "- $a = \\log \\frac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} = \\sum_{i=0}^M w_i x^i$ – это отношение мы моделируем линейной функцией от признаков объекта и параметров модели\n",
    "\n",
    "Данное выражение легко обобщить до множества из $K$ классов, изменится только знаменатель в формуле Байеса. Запишем вероятность принадлежности объекта к классу $k$:\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = k \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right)}{\\sum_{i=1}^K p\\left(\\vec{x} \\mid c = i\\right)p\\left(c = i\\right)} \\\\\n",
    "&=& \\dfrac{e^{z_k}}{\\sum_{i=1}^{K}e^{z_i}} \\\\\n",
    "&=& \\sigma_k\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\sigma_k$ – обозначение функции softmax при векторном аргументе\n",
    "- $z_k = \\log p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right) = \\sum_{i=0}^M w_{ki} x^i$ – это выражение моделируется линейной функций от признаков объекта и параметров класса $k$\n",
    "\n",
    "Для моделирования полного правдоподобия примера мы используем [категориальное распределение](https://en.wikipedia.org/wiki/Categorical_distribution), а лучше его логарифм (для удобства):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\mathcal{L} = \\log p\\left({\\vec{x}}\\right) &=& \\log \\prod_{i=1}^K \\sigma_i\\left(\\vec{z}\\right)^{y_i} \\\\\n",
    "&=& \\sum_{i=1}^K y_i \\log \\sigma_i\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Получается хорошо знакомая нам функция [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) (если домножить на $-1$). Правдоподобие нужно максимизировать, а, соответственно, перекрестную энтропию нужно минимизировать. Продифференцировав по параметрам модели, мы _легко_ получим правила обновления весов для градиентного спуска, **проделайте этот вывод, если вы его не делали** (если вы вдруг сдались, то на [этом](https://www.youtube.com/watch?v=-WiR16raQf4) видео есть разбор вывода, понимание этого вам понадобится для дальнейшего выполнения задания):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} &=& x_m \\left(y_k - \\sigma_k\\left(\\vec{z}\\right)\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "В стандартной формулировке получается, что вектор $\\left(\\sigma_1, \\sigma_2, \\ldots, \\sigma_K\\right)$ образует дискретное вероятностное распределение, т.е. $\\sum_{i=1}^K \\sigma_i = 1$. Но в нашей постановке задачи каждый пример может иметь несколько тегов или одновременно принадлежать к нескольким классам. Для этого мы немного изменим модель:\n",
    "- будем считать, что все теги независимы друг от друга, т.е. каждый исход – это логистическая регрессия на два класса (либо есть тег, либо его нет), тогда вероятность наличия тега у примера запишется следующим образом (каждый тег/класс как и в многоклассовой логрегрессии имеет свой набор параметров):\n",
    "$$\\large p\\left(\\text{tag}_k \\mid \\vec{x}\\right) = \\sigma\\left(z_k\\right) = \\sigma\\left(\\sum_{i=1}^M w_{ki} x^i \\right)$$\n",
    "- наличие каждого тега мы будем моделировать с помощью <a href=\"https://en.wikipedia.org/wiki/Bernoulli_distribution\">распределения Бернулли</a>\n",
    "\n",
    "Ваше первое задание –  записать упрощенное выражение логарифма правдоподобия примера с признаками $\\vec{x}$. Как правило, многие алгоритмы оптимизации имеют интерфейс для минимизации функции, мы последуем этой же традиции, и домножим полученное выражение на $-1$, а во второй части выведем формулы для минимизации полученного выражения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. $\\large -\\mathcal{L} = -\\sum_{i=1}^M y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "2. $\\large -\\mathcal{L} = -\\sum_{i=1}^K y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "3. $\\large -\\mathcal{L} = -\\sum_{i=1}^K z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$\n",
    "4. $\\large -\\mathcal{L} = -\\sum_{i=1}^M z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Вывод формулы обновления весов\n",
    "\n",
    "В качестве второго задания вам предоставляется возможность вывести формулу градиента для $-\\mathcal{L}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(\\sigma\\left(z_k\\right) - y_k\\right)$\n",
    "2. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(y_k - \\sigma\\left(z_k\\right)\\right)$\n",
    "3. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(\\sigma\\left(z_k\\right)x_m - y_k\\right)$\n",
    "4. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(y_k - \\sigma\\left(z_k\\right)x_m\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Имплементация базовой модели\n",
    "\n",
    "Вам предлагается каркас класса модели, разберите его внимательно, обращайте внимание на комментарии. Затем заполните пропуски, запустите полученную модель и ответьте на проверочный вопрос.\n",
    "\n",
    "Как вы могли уже заметить, при обновлении веса $w_{km}$ используется значение признака $x_m$, который равен $0$ если слова с индексом $m$ нет в предложении, и больше нуля, если такое слово есть. Соответственно, при вычислении линейной комбинации $z$ весов модели и признаков примера необходимо учитывать только ненулевые признаки объекта.\n",
    "\n",
    "Подсказка:\n",
    "- если реализовывать вычисление сигмоида так же, как в формуле, то при большом отрицательном значении $z$ вычисление $e^{-z}$ превратится в очень большое число, которое вылетит за допустимые пределы\n",
    "- в то же время $e^{-z}$ от большого положительного $z$ будет нулем\n",
    "- воспользуйтесь свойствами функции $\\sigma$ для того, чтобы пофиксить эту ошибку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags_top : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = 0\n",
    "            \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                            \n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        self._w[tag][self._vocab[word]] += 1  # increment counter\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "                        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "                        #        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "   \n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sigma = 1/(1 + np.power(np.exp(1), -z))\n",
    "    \n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sample_loss += (sigma - y) * len(sentence)\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        if sigma < tolerance:\n",
    "                            sigma = tolerance\n",
    "                        if  sigma > 1 - tolerance:\n",
    "                            sigma = 1 - tolerance\n",
    "                            \n",
    "                        dLdw = y * np.log(sigma) + (1 - y) * np.log(1 - sigma)\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate * dLdw\n",
    "                        self._b[tag] -= -learning_rate * dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc11d1f9567e44f3951bf33c5944bfa0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:99: RuntimeWarning: overflow encountered in power\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# создадим эксемпляр модели и пройдемся по датасету\n",
    "model = LogRegressor()\n",
    "model.iterate_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, действительно ли значение отрицательного логарифмического правдоподобия уменьшалось. Так как мы используем стохастический градентный спуск, не стоит ожидать плавного падения функции ошибки. Мы воспользуемся скользящим средним с окном в 10 000 примеров, чтобы хоть как то сгладить график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeYAAAFKCAYAAADITfxaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIvIj/w8McYFlXLa0aQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f946260e748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the loss function on the last 10k train samples: 79.45\n"
     ]
    }
   ],
   "source": [
    "print('Mean of the loss function on the last 10k train samples: %0.2f' % np.mean(model._loss[-35000:-25000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислите среднее значение функции стоимости на последних 10 000 примеров тренировочного набора, к какому из значений ваш ответ ближе всего?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. 17.54\n",
    "2. 18.64\n",
    "3. 19.74\n",
    "4. 20.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Тестирование модели\n",
    "\n",
    "В базовой модели первые 100 000 строк используются для обучения, а оставшиеся – для тестирования. Как вы можете заметить, значение отрицательного логарифмического правдоподобия не очень информативно, хоть и позволяет сравнивать разные модели. В качестве четвертого задания вам необходимо модифицировать базовую модель таким образом, чтобы метод `iterate_file` возвращал значение _точности_ на тестовой части набора данных. \n",
    "\n",
    "Точность определим следующим образом:\n",
    "- считаем, что тег у вопроса присутствует, если спрогнозированная вероятность тега больше 0.9\n",
    "- точность одного примера расчитывается как [коэффициент Жаккара](https://ru.wikipedia.org/wiki/Коэффициент_Жаккара) между множеством настоящих тегов и предсказанных моделью\n",
    "  - например, если у примера настоящие теги ['html', 'jquery'], а по версии модели ['ios', 'html', 'java'], то коэффициент Жаккара будет равен |['html', 'jquery'] $\\cap$ ['ios', 'html', 'java']| / |['html', 'jquery'] $\\cup$ ['ios', 'html', 'java']| = |['html']| / |['jquery', 'ios', 'html', 'java']| = 1/4\n",
    "- метод `iterate_file` возвращает **среднюю** точность на тестовом наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "# выведем полученное значение с точностью до двух знаков\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ответьте на вопрос,  к какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. 0.39\n",
    "2. 0.49\n",
    "3. 0.59\n",
    "4. 0.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. $L_2$-регуляризация\n",
    "\n",
    "В качестве пятого задания вам необходимо добавить в класс `LogRegressor` поддержку $L_2$-регуляризации. В методе `iterate_file` должен появиться параметр `lmbda=0.01` со значением по умолчанию. С учетом регуляризации новая функция стоимости примет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\frac{\\lambda}{2} \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2\n",
    "\\end{array}$$\n",
    "\n",
    "Градиент первого члена суммы мы уже вывели, а для второго он имеет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial}{\\partial w_{ki}} \\frac{\\lambda}{2} R\\left(W\\right) &=& \\lambda w_{ki}\n",
    "\\end{array}$$\n",
    "\n",
    "Если мы на каждом примере будем делать честное обновление всех весов, то все очень замедлится, ведь нам придется на каждой итерации пробегать по всем словам словаря. В ущерб теоретической точности вы используем грязный трюк: мы будем регуляризаровать только те слова, которые присутствуют в текущем предложении. Не забывайте, что смещение не регуляризируется. `sample_loss` тоже должен остаться без изменений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на вопрос, к какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. 0.3\n",
    "2. 0.35\n",
    "3. 0.4\n",
    "4. 0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ElasticNet регуляризация, вывод\n",
    "Помимо $L_2$ регуляризации, часто используется $L_1$ регуляризация.\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right|\n",
    "\\end{array}$$\n",
    "\n",
    "Если линейно объединить $L_1$ и $L_2$ регуляризацию, то полученный тип регуляризации называется ElasticNet:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\lambda R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\left(\\gamma \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2 + \\left(1 - \\gamma\\right) \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right| \\right)\n",
    "\\end{array}$$\n",
    "- где $\\gamma \\in \\left[0, 1\\right]$\n",
    "\n",
    "В качестве шестого вопроса вам предлагается вывести формулу градиента ElasticNet регуляризации (не учитывая $-\\mathcal{L}$). \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) w_{ki}\\right)$ \n",
    "2. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma \\left|w_{ki}\\right| + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "3. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "4. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(\\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ElasticNet регуляризация, имплементация\n",
    "\n",
    "В качестве седьмой задачи, вам предлается изменить класс `LogRegressor` таким образом, чтобы метод `iterate_file` принимал два параметра со значениями по умолчанию `lmbda=0.0002` и `gamma=0.1`. Сделайте один проход по датасету с включенной ElasticNet регуляризацией и заданными значениями по умолчанию и ответьте на вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ответьте на вопрос,  к какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.59\n",
    "2. 0.69\n",
    "3. 0.79\n",
    "4. 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Самые важные слова для тега\n",
    "\n",
    "Прелесть линейных моделей в том, что они легко интерпретируемы. Вам предлагается вычислить, какие слова вносят наибольший вклад в вероятность каждого из тегов. А затем ответьте на контрольный вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для многих тегов наличие самого тега в предложении является важным сигналом, у многих сам тег является самым сильным сигналом, что неудивительно. Для каких из тегов само название тега не входит в топ-5 самых важных?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. java, c#\n",
    "2. php, javascript\n",
    "3. html, jquery\n",
    "4. ios, android"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9. Сокращаем размер словаря\n",
    "Сейчас количество слов в словаре около 90 000, если бы это была выборка из 10 миллионов вопросов с сайта StackOverflow, то размер словаря был бы миллионов 10. Регуляризировать модель можно не только изящно математически, но и топорно, например, ограничить размер словаря. Вам предоставляется возможность внести следующие изменения в класс `LogRegressor`:\n",
    "- добавить в метод `iterate_file` еще один аргумент со значением по умолчанию `update_vocab=True`\n",
    "- при `update_vocab=True` разрешать добавлять слова в словарь в режиме обучения\n",
    "- при `update_vocab=False` игнорировать слова не из словаря\n",
    "- добавить в класс метод `filter_vocab(n=10000)`, который оставит в словаре только топ-n самых популярных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# оставим только топ 10 000 слов\n",
    "model.filter_vocab(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# сделаем еще одну итерацию по датасету, уменьшив скорость обучения в 10 раз\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ответьте на вопрос,  к какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.48\n",
    "2. 0.58\n",
    "3. 0.68\n",
    "4. 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Прогнозирование тегов для новых вопросов\n",
    "\n",
    "В завершение сегодняшней домашки, вам предлагается реализовать метод `predict_proba`, который принимает строку,  содержащую вопрос, а возвращает список предсказанных тегов вопроса с их вероятностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "model.filter_vocab(n=10000)\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = (\"I want to improve my coding skills, so I have planned write \" +\n",
    "            \"a Mobile Application.need to choose between Apple's iOS or Google's Android.\" +\n",
    "            \" my background: I have done basic programming in .Net,C/C++,Python and PHP \" +\n",
    "            \"in college, so got OOP concepts covered. about my skill level, I just know \" +\n",
    "            \"concepts and basic syntax. But can't write complex applications, if asked :(\" +\n",
    "            \" So decided to hone my skills, And I wanted to know which is easier to \" +\n",
    "            \"learn for a programming n00b. A) iOS which uses Objective C B) Android \" + \n",
    "            \"which uses Java. I want to decide based on difficulty level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на вопрос, какой или какие теги ассоциируются с данным вопросом, если порог принятия равен $0.9$?:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. android\n",
    "2. ios\n",
    "3. ios, php\n",
    "4. c#, c++, ods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

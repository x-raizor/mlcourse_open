{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению. Сессия № 2\n",
    "Авторы материала: Павел Нестеров. Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Домашняя работа №4\n",
    "## <center> Логистическая регрессия в задаче тегирования вопросов StackOverflow\n",
    "\n",
    "**Надо вывести формулы, где это просится (да, ручка и бумажка), заполнить код в клетках и выбрать ответы в [веб-форме](https://docs.google.com/forms/d/1I_ticU8rpeoGJjsBUcaInpvgdxdq60hV7IcSvo4rlGo/).**\n",
    "\n",
    "## 0. Описание задачи\n",
    "\n",
    "В этой домашней работе мы с вами изучим и запрограммируем модель для прогнозирования тегов по тексту вопроса на базе многоклассовой логистической регрессии. В отличие от обычной постановки задачи классификации (multiclass), в данном случае один пример может принадлежать одновременно к нескольким классам (multilabel). Мы будем реализовывать онлайн версию алгоритма мультиклассовой классификации.\n",
    "\n",
    "Мы будем использовать небольшую выборку из протеггированных вопросов с сайта StackOverflow размером в 125 тысяч примеров (около 150 Мб, скачайте по [этой](https://drive.google.com/open?id=0B4bl7YMqDnViYVo0V2FubFVhMFE) ссылке).\n",
    "\n",
    "PS: Можно показать, что такая реализация совсем не эффективная и проще было бы использовать векторизированные вычисления. Для данного датасета так и есть. Но на самом деле подобные реализации используются в жизни, но естественно, написаны они не на Python. Например, в онлайн моделях прогнозирования [CTR](https://en.wikipedia.org/wiki/Click-through_rate) юзеру показывается баннер, затем в зависимости от наличия клика происходит обновление параметров модели. В реальной жизни параметров модели может быть несколько сотен миллионов, а у юзера из этих ста миллионов от силы сто или тысяча параметров отличны от нуля, векторизировать такие вычисления не очень эффективно. Обычно все это хранится в огромных кластерах в in-memory базах данных, а обработка пользователей происходит распределенно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "sns.set(font='DejaVu Sans')\n",
    "\n",
    "# поменяйте на свой путь\n",
    "DS_FILE_NAME = \"../../data/stackoverflow/stackoverflow_sample_125k.tsv\"\n",
    "TAGS_FILE_NAME = \"../../data/stackoverflow/top10_tags.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python', 'jquery', 'html', 'android', 'php', 'c++', 'java', 'javascript', 'c#', 'ios'}\n"
     ]
    }
   ],
   "source": [
    "top_tags = []\n",
    "\n",
    "with open(TAGS_FILE_NAME, 'r') as f:\n",
    "    for line in f:\n",
    "        top_tags.append(line.strip())\n",
    "\n",
    "top_tags = set(top_tags)\n",
    "\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Многоклассовая логистическая регрессия\n",
    "\n",
    "Вспомним, как получается логистическая регрессия для двух классов $\\left\\{0, 1\\right\\}$, вероятность принадлежности объекта к классу $1$ выписывается по теореме Байеса:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = 1 \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right) + p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} \\\\\n",
    "&=& \\dfrac{1}{1 + e^{-a}} \\\\\n",
    "&=& \\sigma\\left(a\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\vec{x}$ – вектор признаков объекта\n",
    "- $\\sigma$ – обозначение функции логистического сигмоида при скалярном аргументе\n",
    "- $a = \\log \\frac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} = \\sum_{i=0}^M w_i x^i$ – это отношение мы моделируем линейной функцией от признаков объекта и параметров модели\n",
    "\n",
    "Данное выражение легко обобщить до множества из $K$ классов, изменится только знаменатель в формуле Байеса. Запишем вероятность принадлежности объекта к классу $k$:\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = k \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right)}{\\sum_{i=1}^K p\\left(\\vec{x} \\mid c = i\\right)p\\left(c = i\\right)} \\\\\n",
    "&=& \\dfrac{e^{z_k}}{\\sum_{i=1}^{K}e^{z_i}} \\\\\n",
    "&=& \\sigma_k\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\sigma_k$ – обозначение функции softmax при векторном аргументе\n",
    "- $z_k = \\log p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right) = \\sum_{i=0}^M w_{ki} x^i$ – это выражение моделируется линейной функций от признаков объекта и параметров класса $k$\n",
    "\n",
    "Для моделирования полного правдоподобия примера мы используем [категориальное распределение](https://en.wikipedia.org/wiki/Categorical_distribution), а лучше его логарифм (для удобства):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\mathcal{L} = \\log p\\left({\\vec{x}}\\right) &=& \\log \\prod_{i=1}^K \\sigma_i\\left(\\vec{z}\\right)^{y_i} \\\\\n",
    "&=& \\sum_{i=1}^K y_i \\log \\sigma_i\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Получается хорошо знакомая нам функция [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) (если домножить на $-1$). Правдоподобие нужно максимизировать, а, соответственно, перекрестную энтропию нужно минимизировать. Продифференцировав по параметрам модели, мы _легко_ получим правила обновления весов для градиентного спуска, **проделайте этот вывод, если вы его не делали** (если вы вдруг сдались, то на [этом](https://www.youtube.com/watch?v=-WiR16raQf4) видео есть разбор вывода, понимание этого вам понадобится для дальнейшего выполнения задания):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} &=& x_m \\left(y_k - \\sigma_k\\left(\\vec{z}\\right)\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "В стандартной формулировке получается, что вектор $\\left(\\sigma_1, \\sigma_2, \\ldots, \\sigma_K\\right)$ образует дискретное вероятностное распределение, т.е. $\\sum_{i=1}^K \\sigma_i = 1$. Но в нашей постановке задачи каждый пример может иметь несколько тегов или одновременно принадлежать к нескольким классам. Для этого мы немного изменим модель:\n",
    "- будем считать, что все теги независимы друг от друга, т.е. каждый исход – это логистическая регрессия на два класса (либо есть тег, либо его нет), тогда вероятность наличия тега у примера запишется следующим образом (каждый тег/класс как и в многоклассовой логрегрессии имеет свой набор параметров):\n",
    "$$\\large p\\left(\\text{tag}_k \\mid \\vec{x}\\right) = \\sigma\\left(z_k\\right) = \\sigma\\left(\\sum_{i=1}^M w_{ki} x^i \\right)$$\n",
    "- наличие каждого тега мы будем моделировать с помощью <a href=\"https://en.wikipedia.org/wiki/Bernoulli_distribution\">распределения Бернулли</a>\n",
    "\n",
    "Ваше первое задание –  записать упрощенное выражение логарифма правдоподобия примера с признаками $\\vec{x}$. Как правило, многие алгоритмы оптимизации имеют интерфейс для минимизации функции, мы последуем этой же традиции, и домножим полученное выражение на $-1$, а во второй части выведем формулы для минимизации полученного выражения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. $\\large -\\mathcal{L} = -\\sum_{i=1}^M y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "2. $\\large -\\mathcal{L} = -\\sum_{i=1}^K y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "3. $\\large -\\mathcal{L} = -\\sum_{i=1}^K z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$\n",
    "4. $\\large -\\mathcal{L} = -\\sum_{i=1}^M z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Вывод формулы обновления весов\n",
    "\n",
    "В качестве второго задания вам предоставляется возможность вывести формулу градиента для $-\\mathcal{L}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(\\sigma\\left(z_k\\right) - y_k\\right)$\n",
    "2. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(y_k - \\sigma\\left(z_k\\right)\\right)$\n",
    "3. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(\\sigma\\left(z_k\\right)x_m - y_k\\right)$\n",
    "4. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(y_k - \\sigma\\left(z_k\\right)x_m\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Имплементация базовой модели\n",
    "\n",
    "Вам предлагается каркас класса модели, разберите его внимательно, обращайте внимание на комментарии. Затем заполните пропуски, запустите полученную модель и ответьте на проверочный вопрос.\n",
    "\n",
    "Как вы могли уже заметить, при обновлении веса $w_{km}$ используется значение признака $x_m$, который равен $0$ если слова с индексом $m$ нет в предложении, и больше нуля, если такое слово есть. Соответственно, при вычислении линейной комбинации $z$ весов модели и признаков примера необходимо учитывать только ненулевые признаки объекта.\n",
    "\n",
    "Подсказка:\n",
    "- если реализовывать вычисление сигмоида так же, как в формуле, то при большом отрицательном значении $z$ вычисление $e^{-z}$ превратится в очень большое число, которое вылетит за допустимые пределы\n",
    "- в то же время $e^{-z}$ от большого положительного $z$ будет нулем\n",
    "- воспользуйтесь свойствами функции $\\sigma$ для того, чтобы пофиксить эту ошибку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags_top : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "            \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                            \n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        z += self._w[tag][self._vocab[word]]   \n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = max(-709.78, z)\n",
    "                    sigma = 1/(1 + np.power(np.exp(1), -z))\n",
    "                      \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    log_sigma = np.log(sigma) if sigma > tolerance else np.log(tolerance)\n",
    "                    log_one_min_sigma = np.log(1 - sigma) \\\n",
    "                            if (1 - sigma) > tolerance \\\n",
    "                            else np.log(tolerance)\n",
    "\n",
    "                    sample_loss += -(y * log_sigma + (1 - y) * log_one_min_sigma)\n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = (y - sigma) #* len(sentence)  # +1\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate * dLdw\n",
    "                        self._b[tag] -= -learning_rate * dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d44f760ead40ec9700cb4a08bfa122"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# создадим эксемпляр модели и пройдемся по датасету\n",
    "model = LogRegressor()\n",
    "model.iterate_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, действительно ли значение отрицательного логарифмического правдоподобия уменьшалось. Так как мы используем стохастический градентный спуск, не стоит ожидать плавного падения функции ошибки. Мы воспользуемся скользящим средним с окном в 10 000 примеров, чтобы хоть как то сгладить график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFKCAYAAADFU4wdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE7HLx3+2GXYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe2ccdd62b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the loss function on the last 10k train samples: 20.63\n"
     ]
    }
   ],
   "source": [
    "print('Mean of the loss function on the last 10k train samples: %0.2f' % np.mean(model._loss[-35000:-25000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислите среднее значение функции стоимости на последних 10 000 примеров тренировочного набора, к какому из значений ваш ответ ближе всего?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. 17.54\n",
    "2. 18.64\n",
    "3. 19.74\n",
    "4. 20.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Тестирование модели\n",
    "\n",
    "В базовой модели первые 100 000 строк используются для обучения, а оставшиеся – для тестирования. Как вы можете заметить, значение отрицательного логарифмического правдоподобия не очень информативно, хоть и позволяет сравнивать разные модели. В качестве четвертого задания вам необходимо модифицировать базовую модель таким образом, чтобы метод `iterate_file` возвращал значение _точности_ на тестовой части набора данных. \n",
    "\n",
    "Точность определим следующим образом:\n",
    "- считаем, что тег у вопроса присутствует, если спрогнозированная вероятность тега больше 0.9\n",
    "- точность одного примера расчитывается как [коэффициент Жаккара](https://ru.wikipedia.org/wiki/Коэффициент_Жаккара) между множеством настоящих тегов и предсказанных моделью\n",
    "  - например, если у примера настоящие теги ['html', 'jquery'], а по версии модели ['ios', 'html', 'java'], то коэффициент Жаккара будет равен |['html', 'jquery'] $\\cap$ ['ios', 'html', 'java']| / |['html', 'jquery'] $\\cup$ ['ios', 'html', 'java']| = |['html']| / |['jquery', 'ios', 'html', 'java']| = 1/4\n",
    "- метод `iterate_file` возвращает **среднюю** точность на тестовом наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "# выведем полученное значение с точностью до двух знаков\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ответьте на вопрос,  к какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. 0.39\n",
    "2. 0.49\n",
    "3. 0.59\n",
    "4. 0.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. $L_2$-регуляризация\n",
    "\n",
    "В качестве пятого задания вам необходимо добавить в класс `LogRegressor` поддержку $L_2$-регуляризации. В методе `iterate_file` должен появиться параметр `lmbda=0.01` со значением по умолчанию. С учетом регуляризации новая функция стоимости примет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\frac{\\lambda}{2} \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2\n",
    "\\end{array}$$\n",
    "\n",
    "Градиент первого члена суммы мы уже вывели, а для второго он имеет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial}{\\partial w_{ki}} \\frac{\\lambda}{2} R\\left(W\\right) &=& \\lambda w_{ki}\n",
    "\\end{array}$$\n",
    "\n",
    "Если мы на каждом примере будем делать честное обновление всех весов, то все очень замедлится, ведь нам придется на каждой итерации пробегать по всем словам словаря. В ущерб теоретической точности вы используем грязный трюк: мы будем регуляризаровать только те слова, которые присутствуют в текущем предложении. Не забывайте, что смещение не регуляризируется. `sample_loss` тоже должен остаться без изменений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на вопрос, к какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. 0.3\n",
    "2. 0.35\n",
    "3. 0.4\n",
    "4. 0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ElasticNet регуляризация, вывод\n",
    "Помимо $L_2$ регуляризации, часто используется $L_1$ регуляризация.\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right|\n",
    "\\end{array}$$\n",
    "\n",
    "Если линейно объединить $L_1$ и $L_2$ регуляризацию, то полученный тип регуляризации называется ElasticNet:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\lambda R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\left(\\gamma \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2 + \\left(1 - \\gamma\\right) \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right| \\right)\n",
    "\\end{array}$$\n",
    "- где $\\gamma \\in \\left[0, 1\\right]$\n",
    "\n",
    "В качестве шестого вопроса вам предлагается вывести формулу градиента ElasticNet регуляризации (не учитывая $-\\mathcal{L}$). \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) w_{ki}\\right)$ \n",
    "2. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma \\left|w_{ki}\\right| + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "3. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "4. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(\\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ElasticNet регуляризация, имплементация\n",
    "\n",
    "В качестве седьмой задачи, вам предлается изменить класс `LogRegressor` таким образом, чтобы метод `iterate_file` принимал два параметра со значениями по умолчанию `lmbda=0.0002` и `gamma=0.1`. Сделайте один проход по датасету с включенной ElasticNet регуляризацией и заданными значениями по умолчанию и ответьте на вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ответьте на вопрос,  к какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.59\n",
    "2. 0.69\n",
    "3. 0.79\n",
    "4. 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Самые важные слова для тега\n",
    "\n",
    "Прелесть линейных моделей в том, что они легко интерпретируемы. Вам предлагается вычислить, какие слова вносят наибольший вклад в вероятность каждого из тегов. А затем ответьте на контрольный вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для многих тегов наличие самого тега в предложении является важным сигналом, у многих сам тег является самым сильным сигналом, что неудивительно. Для каких из тегов само название тега не входит в топ-5 самых важных?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. java, c#\n",
    "2. php, javascript\n",
    "3. html, jquery\n",
    "4. ios, android"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9. Сокращаем размер словаря\n",
    "Сейчас количество слов в словаре около 90 000, если бы это была выборка из 10 миллионов вопросов с сайта StackOverflow, то размер словаря был бы миллионов 10. Регуляризировать модель можно не только изящно математически, но и топорно, например, ограничить размер словаря. Вам предоставляется возможность внести следующие изменения в класс `LogRegressor`:\n",
    "- добавить в метод `iterate_file` еще один аргумент со значением по умолчанию `update_vocab=True`\n",
    "- при `update_vocab=True` разрешать добавлять слова в словарь в режиме обучения\n",
    "- при `update_vocab=False` игнорировать слова не из словаря\n",
    "- добавить в класс метод `filter_vocab(n=10000)`, который оставит в словаре только топ-n самых популярных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# оставим только топ 10 000 слов\n",
    "model.filter_vocab(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# сделаем еще одну итерацию по датасету, уменьшив скорость обучения в 10 раз\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ответьте на вопрос,  к какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.48\n",
    "2. 0.58\n",
    "3. 0.68\n",
    "4. 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Прогнозирование тегов для новых вопросов\n",
    "\n",
    "В завершение сегодняшней домашки, вам предлагается реализовать метод `predict_proba`, который принимает строку,  содержащую вопрос, а возвращает список предсказанных тегов вопроса с их вероятностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "model.filter_vocab(n=10000)\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = (\"I want to improve my coding skills, so I have planned write \" +\n",
    "            \"a Mobile Application.need to choose between Apple's iOS or Google's Android.\" +\n",
    "            \" my background: I have done basic programming in .Net,C/C++,Python and PHP \" +\n",
    "            \"in college, so got OOP concepts covered. about my skill level, I just know \" +\n",
    "            \"concepts and basic syntax. But can't write complex applications, if asked :(\" +\n",
    "            \" So decided to hone my skills, And I wanted to know which is easier to \" +\n",
    "            \"learn for a programming n00b. A) iOS which uses Objective C B) Android \" + \n",
    "            \"which uses Java. I want to decide based on difficulty level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на вопрос, какой или какие теги ассоциируются с данным вопросом, если порог принятия равен $0.9$?:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. android\n",
    "2. ios\n",
    "3. ios, php\n",
    "4. c#, c++, ods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

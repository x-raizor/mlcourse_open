{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению. Сессия № 2\n",
    "Авторы материала: Павел Нестеров. Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Домашняя работа №4\n",
    "## <center> Логистическая регрессия в задаче тегирования вопросов StackOverflow\n",
    "\n",
    "**Надо вывести формулы, где это просится (да, ручка и бумажка), заполнить код в клетках и выбрать ответы в [веб-форме](https://docs.google.com/forms/d/1I_ticU8rpeoGJjsBUcaInpvgdxdq60hV7IcSvo4rlGo/).**\n",
    "\n",
    "## 0. Описание задачи\n",
    "\n",
    "В этой домашней работе мы с вами изучим и запрограммируем модель для прогнозирования тегов по тексту вопроса на базе многоклассовой логистической регрессии. В отличие от обычной постановки задачи классификации (multiclass), в данном случае один пример может принадлежать одновременно к нескольким классам (multilabel). Мы будем реализовывать онлайн версию алгоритма мультиклассовой классификации.\n",
    "\n",
    "Мы будем использовать небольшую выборку из протеггированных вопросов с сайта StackOverflow размером в 125 тысяч примеров (около 150 Мб, скачайте по [этой](https://drive.google.com/open?id=0B4bl7YMqDnViYVo0V2FubFVhMFE) ссылке).\n",
    "\n",
    "PS: Можно показать, что такая реализация совсем не эффективная и проще было бы использовать векторизированные вычисления. Для данного датасета так и есть. Но на самом деле подобные реализации используются в жизни, но естественно, написаны они не на Python. Например, в онлайн моделях прогнозирования [CTR](https://en.wikipedia.org/wiki/Click-through_rate) юзеру показывается баннер, затем в зависимости от наличия клика происходит обновление параметров модели. В реальной жизни параметров модели может быть несколько сотен миллионов, а у юзера из этих ста миллионов от силы сто или тысяча параметров отличны от нуля, векторизировать такие вычисления не очень эффективно. Обычно все это хранится в огромных кластерах в in-memory базах данных, а обработка пользователей происходит распределенно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "sns.set(font='DejaVu Sans')\n",
    "\n",
    "# поменяйте на свой путь\n",
    "DS_FILE_NAME = \"../../data/stackoverflow/stackoverflow_sample_125k.tsv\"\n",
    "TAGS_FILE_NAME = \"../../data/stackoverflow/top10_tags.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python', 'javascript', 'html', 'java', 'ios', 'android', 'php', 'c++', 'jquery', 'c#'}\n"
     ]
    }
   ],
   "source": [
    "top_tags = []\n",
    "\n",
    "with open(TAGS_FILE_NAME, 'r') as f:\n",
    "    for line in f:\n",
    "        top_tags.append(line.strip())\n",
    "\n",
    "top_tags = set(top_tags)\n",
    "\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Многоклассовая логистическая регрессия\n",
    "\n",
    "Вспомним, как получается логистическая регрессия для двух классов $\\left\\{0, 1\\right\\}$, вероятность принадлежности объекта к классу $1$ выписывается по теореме Байеса:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = 1 \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right) + p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} \\\\\n",
    "&=& \\dfrac{1}{1 + e^{-a}} \\\\\n",
    "&=& \\sigma\\left(a\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\vec{x}$ – вектор признаков объекта\n",
    "- $\\sigma$ – обозначение функции логистического сигмоида при скалярном аргументе\n",
    "- $a = \\log \\frac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} = \\sum_{i=0}^M w_i x^i$ – это отношение мы моделируем линейной функцией от признаков объекта и параметров модели\n",
    "\n",
    "Данное выражение легко обобщить до множества из $K$ классов, изменится только знаменатель в формуле Байеса. Запишем вероятность принадлежности объекта к классу $k$:\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = k \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right)}{\\sum_{i=1}^K p\\left(\\vec{x} \\mid c = i\\right)p\\left(c = i\\right)} \\\\\n",
    "&=& \\dfrac{e^{z_k}}{\\sum_{i=1}^{K}e^{z_i}} \\\\\n",
    "&=& \\sigma_k\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\sigma_k$ – обозначение функции softmax при векторном аргументе\n",
    "- $z_k = \\log p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right) = \\sum_{i=0}^M w_{ki} x^i$ – это выражение моделируется линейной функций от признаков объекта и параметров класса $k$\n",
    "\n",
    "Для моделирования полного правдоподобия примера мы используем [категориальное распределение](https://en.wikipedia.org/wiki/Categorical_distribution), а лучше его логарифм (для удобства):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\mathcal{L} = \\log p\\left({\\vec{x}}\\right) &=& \\log \\prod_{i=1}^K \\sigma_i\\left(\\vec{z}\\right)^{y_i} \\\\\n",
    "&=& \\sum_{i=1}^K y_i \\log \\sigma_i\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Получается хорошо знакомая нам функция [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) (если домножить на $-1$). Правдоподобие нужно максимизировать, а, соответственно, перекрестную энтропию нужно минимизировать. Продифференцировав по параметрам модели, мы _легко_ получим правила обновления весов для градиентного спуска, **проделайте этот вывод, если вы его не делали** (если вы вдруг сдались, то на [этом](https://www.youtube.com/watch?v=-WiR16raQf4) видео есть разбор вывода, понимание этого вам понадобится для дальнейшего выполнения задания):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} &=& x_m \\left(y_k - \\sigma_k\\left(\\vec{z}\\right)\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "В стандартной формулировке получается, что вектор $\\left(\\sigma_1, \\sigma_2, \\ldots, \\sigma_K\\right)$ образует дискретное вероятностное распределение, т.е. $\\sum_{i=1}^K \\sigma_i = 1$. Но в нашей постановке задачи каждый пример может иметь несколько тегов или одновременно принадлежать к нескольким классам. Для этого мы немного изменим модель:\n",
    "- будем считать, что все теги независимы друг от друга, т.е. каждый исход – это логистическая регрессия на два класса (либо есть тег, либо его нет), тогда вероятность наличия тега у примера запишется следующим образом (каждый тег/класс как и в многоклассовой логрегрессии имеет свой набор параметров):\n",
    "$$\\large p\\left(\\text{tag}_k \\mid \\vec{x}\\right) = \\sigma\\left(z_k\\right) = \\sigma\\left(\\sum_{i=1}^M w_{ki} x^i \\right)$$\n",
    "- наличие каждого тега мы будем моделировать с помощью <a href=\"https://en.wikipedia.org/wiki/Bernoulli_distribution\">распределения Бернулли</a>\n",
    "\n",
    "Ваше первое задание –  записать упрощенное выражение логарифма правдоподобия примера с признаками $\\vec{x}$. Как правило, многие алгоритмы оптимизации имеют интерфейс для минимизации функции, мы последуем этой же традиции, и домножим полученное выражение на $-1$, а во второй части выведем формулы для минимизации полученного выражения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. $\\large -\\mathcal{L} = -\\sum_{i=1}^M y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "2. $\\large -\\mathcal{L} = -\\sum_{i=1}^K y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "3. $\\large -\\mathcal{L} = -\\sum_{i=1}^K z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$\n",
    "4. $\\large -\\mathcal{L} = -\\sum_{i=1}^M z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Вывод формулы обновления весов\n",
    "\n",
    "В качестве второго задания вам предоставляется возможность вывести формулу градиента для $-\\mathcal{L}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(\\sigma\\left(z_k\\right) - y_k\\right)$\n",
    "2. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(y_k - \\sigma\\left(z_k\\right)\\right)$\n",
    "3. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(\\sigma\\left(z_k\\right)x_m - y_k\\right)$\n",
    "4. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(y_k - \\sigma\\left(z_k\\right)x_m\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Имплементация базовой модели\n",
    "\n",
    "Вам предлагается каркас класса модели, разберите его внимательно, обращайте внимание на комментарии. Затем заполните пропуски, запустите полученную модель и ответьте на проверочный вопрос.\n",
    "\n",
    "Как вы могли уже заметить, при обновлении веса $w_{km}$ используется значение признака $x_m$, который равен $0$ если слова с индексом $m$ нет в предложении, и больше нуля, если такое слово есть. Соответственно, при вычислении линейной комбинации $z$ весов модели и признаков примера необходимо учитывать только ненулевые признаки объекта.\n",
    "\n",
    "Подсказка:\n",
    "- если реализовывать вычисление сигмоида так же, как в формуле, то при большом отрицательном значении $z$ вычисление $e^{-z}$ превратится в очень большое число, которое вылетит за допустимые пределы\n",
    "- в то же время $e^{-z}$ от большого положительного $z$ будет нулем\n",
    "- воспользуйтесь свойствами функции $\\sigma$ для того, чтобы пофиксить эту ошибку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags_top : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = 0\n",
    "            \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                            \n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        self._w[tag][self._vocab[word]] += 1  # increment counter\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "                        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "                        #        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "   \n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sigma = 1/(1 + np.power(np.exp(1), -z))\n",
    "    \n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sample_loss += (sigma - y) * len(list(set(sentence)))\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        if sigma < tolerance:\n",
    "                            sigma = tolerance\n",
    "                        if  sigma > 1 - tolerance:\n",
    "                            sigma = 1 - tolerance\n",
    "                            \n",
    "                        dLdw = y * np.log(sigma) + (1 - y) * np.log(1 - sigma)\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate * dLdw\n",
    "                        self._b[tag] -= -learning_rate * dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2da64fcb3634a4cbd80b88a66dc87a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:99: RuntimeWarning: overflow encountered in power\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# создадим эксемпляр модели и пройдемся по датасету\n",
    "model = LogRegressor()\n",
    "model.iterate_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, действительно ли значение отрицательного логарифмического правдоподобия уменьшалось. Так как мы используем стохастический градентный спуск, не стоит ожидать плавного падения функции ошибки. Мы воспользуемся скользящим средним с окном в 10 000 примеров, чтобы хоть как то сгладить график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFKCAYAAADFU4wdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkQr+HyX69dkhdJymAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f947f0ccb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the loss function on the last 10k train samples: 33.96\n"
     ]
    }
   ],
   "source": [
    "print('Mean of the loss function on the last 10k train samples: %0.2f' % np.mean(model._loss[-35000:-25000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислите среднее значение функции стоимости на последних 10 000 примеров тренировочного набора, к какому из значений ваш ответ ближе всего?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. 17.54\n",
    "2. 18.64\n",
    "3. 19.74\n",
    "4. 20.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Тестирование модели\n",
    "\n",
    "В базовой модели первые 100 000 строк используются для обучения, а оставшиеся – для тестирования. Как вы можете заметить, значение отрицательного логарифмического правдоподобия не очень информативно, хоть и позволяет сравнивать разные модели. В качестве четвертого задания вам необходимо модифицировать базовую модель таким образом, чтобы метод `iterate_file` возвращал значение _точности_ на тестовой части набора данных. \n",
    "\n",
    "Точность определим следующим образом:\n",
    "- считаем, что тег у вопроса присутствует, если спрогнозированная вероятность тега больше 0.9\n",
    "- точность одного примера расчитывается как [коэффициент Жаккара](https://ru.wikipedia.org/wiki/Коэффициент_Жаккара) между множеством настоящих тегов и предсказанных моделью\n",
    "  - например, если у примера настоящие теги ['html', 'jquery'], а по версии модели ['ios', 'html', 'java'], то коэффициент Жаккара будет равен |['html', 'jquery'] $\\cap$ ['ios', 'html', 'java']| / |['html', 'jquery'] $\\cup$ ['ios', 'html', 'java']| = |['html']| / |['jquery', 'ios', 'html', 'java']| = 1/4\n",
    "- метод `iterate_file` возвращает **среднюю** точность на тестовом наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "# выведем полученное значение с точностью до двух знаков\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ответьте на вопрос,  к какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. 0.39\n",
    "2. 0.49\n",
    "3. 0.59\n",
    "4. 0.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. $L_2$-регуляризация\n",
    "\n",
    "В качестве пятого задания вам необходимо добавить в класс `LogRegressor` поддержку $L_2$-регуляризации. В методе `iterate_file` должен появиться параметр `lmbda=0.01` со значением по умолчанию. С учетом регуляризации новая функция стоимости примет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\frac{\\lambda}{2} \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2\n",
    "\\end{array}$$\n",
    "\n",
    "Градиент первого члена суммы мы уже вывели, а для второго он имеет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial}{\\partial w_{ki}} \\frac{\\lambda}{2} R\\left(W\\right) &=& \\lambda w_{ki}\n",
    "\\end{array}$$\n",
    "\n",
    "Если мы на каждом примере будем делать честное обновление всех весов, то все очень замедлится, ведь нам придется на каждой итерации пробегать по всем словам словаря. В ущерб теоретической точности вы используем грязный трюк: мы будем регуляризаровать только те слова, которые присутствуют в текущем предложении. Не забывайте, что смещение не регуляризируется. `sample_loss` тоже должен остаться без изменений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на вопрос, к какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. 0.3\n",
    "2. 0.35\n",
    "3. 0.4\n",
    "4. 0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ElasticNet регуляризация, вывод\n",
    "Помимо $L_2$ регуляризации, часто используется $L_1$ регуляризация.\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right|\n",
    "\\end{array}$$\n",
    "\n",
    "Если линейно объединить $L_1$ и $L_2$ регуляризацию, то полученный тип регуляризации называется ElasticNet:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\lambda R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\left(\\gamma \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2 + \\left(1 - \\gamma\\right) \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right| \\right)\n",
    "\\end{array}$$\n",
    "- где $\\gamma \\in \\left[0, 1\\right]$\n",
    "\n",
    "В качестве шестого вопроса вам предлагается вывести формулу градиента ElasticNet регуляризации (не учитывая $-\\mathcal{L}$). \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) w_{ki}\\right)$ \n",
    "2. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma \\left|w_{ki}\\right| + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "3. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "4. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(\\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ElasticNet регуляризация, имплементация\n",
    "\n",
    "В качестве седьмой задачи, вам предлается изменить класс `LogRegressor` таким образом, чтобы метод `iterate_file` принимал два параметра со значениями по умолчанию `lmbda=0.0002` и `gamma=0.1`. Сделайте один проход по датасету с включенной ElasticNet регуляризацией и заданными значениями по умолчанию и ответьте на вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ответьте на вопрос,  к какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.59\n",
    "2. 0.69\n",
    "3. 0.79\n",
    "4. 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Самые важные слова для тега\n",
    "\n",
    "Прелесть линейных моделей в том, что они легко интерпретируемы. Вам предлагается вычислить, какие слова вносят наибольший вклад в вероятность каждого из тегов. А затем ответьте на контрольный вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для многих тегов наличие самого тега в предложении является важным сигналом, у многих сам тег является самым сильным сигналом, что неудивительно. Для каких из тегов само название тега не входит в топ-5 самых важных?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. java, c#\n",
    "2. php, javascript\n",
    "3. html, jquery\n",
    "4. ios, android"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9. Сокращаем размер словаря\n",
    "Сейчас количество слов в словаре около 90 000, если бы это была выборка из 10 миллионов вопросов с сайта StackOverflow, то размер словаря был бы миллионов 10. Регуляризировать модель можно не только изящно математически, но и топорно, например, ограничить размер словаря. Вам предоставляется возможность внести следующие изменения в класс `LogRegressor`:\n",
    "- добавить в метод `iterate_file` еще один аргумент со значением по умолчанию `update_vocab=True`\n",
    "- при `update_vocab=True` разрешать добавлять слова в словарь в режиме обучения\n",
    "- при `update_vocab=False` игнорировать слова не из словаря\n",
    "- добавить в класс метод `filter_vocab(n=10000)`, который оставит в словаре только топ-n самых популярных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# оставим только топ 10 000 слов\n",
    "model.filter_vocab(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# сделаем еще одну итерацию по датасету, уменьшив скорость обучения в 10 раз\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ответьте на вопрос,  к какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.48\n",
    "2. 0.58\n",
    "3. 0.68\n",
    "4. 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Прогнозирование тегов для новых вопросов\n",
    "\n",
    "В завершение сегодняшней домашки, вам предлагается реализовать метод `predict_proba`, который принимает строку,  содержащую вопрос, а возвращает список предсказанных тегов вопроса с их вероятностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "model.filter_vocab(n=10000)\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = (\"I want to improve my coding skills, so I have planned write \" +\n",
    "            \"a Mobile Application.need to choose between Apple's iOS or Google's Android.\" +\n",
    "            \" my background: I have done basic programming in .Net,C/C++,Python and PHP \" +\n",
    "            \"in college, so got OOP concepts covered. about my skill level, I just know \" +\n",
    "            \"concepts and basic syntax. But can't write complex applications, if asked :(\" +\n",
    "            \" So decided to hone my skills, And I wanted to know which is easier to \" +\n",
    "            \"learn for a programming n00b. A) iOS which uses Objective C B) Android \" + \n",
    "            \"which uses Java. I want to decide based on difficulty level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на вопрос, какой или какие теги ассоциируются с данным вопросом, если порог принятия равен $0.9$?:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. android\n",
    "2. ios\n",
    "3. ios, php\n",
    "4. c#, c++, ods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
